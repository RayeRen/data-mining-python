{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib\n",
    "from os import walk, path\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word = set()\n",
    "stop = set(stopwords.words('english'))\n",
    "high_freq_word = set(['a', 'the', 'to']) | stop\n",
    "\n",
    "all_file = []\n",
    "for (dirpath, dirnames, filenames) in walk('./'):\n",
    "    dirpath = dirpath.replace('\\\\', \"/\")\n",
    "    for f in filenames:\n",
    "        if f.endswith('.txt') and dirpath.startswith('./data/train'):\n",
    "            all_file.append(path.join(dirpath, f))\n",
    "            \n",
    "for f in all_file:\n",
    "    for x in open(f, encoding='utf-8',errors='ignore').read().lower().split():\n",
    "        if x.isalpha() and len(x) > 1 and x not in all_word:\n",
    "            all_word.add(x)\n",
    "            \n",
    "all_word = all_word - high_freq_word\n",
    "id2word = list(all_word)\n",
    "word2id = dict(list(zip(id2word, list(range(len(id2word))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_ham = np.zeros(len(all_word))\n",
    "x_train_spam = np.zeros(len(all_word))\n",
    "ham_total = 0\n",
    "spam_total = 0\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk('./data/train'):\n",
    "    dirpath = dirpath.replace('\\\\', \"/\")\n",
    "    for f in filenames:\n",
    "        if f.endswith('.txt'):\n",
    "            word_appears = np.zeros(len(all_word))\n",
    "            for x in open(path.join(dirpath, f), encoding='utf-8', errors='ignore').read().lower().split():\n",
    "                if x.isalpha() and x in all_word and len(x) > 1 and word_appears[word2id[x]]==0:\n",
    "                    word_appears[word2id[x]] = 1\n",
    "                    \n",
    "            if dirpath.startswith('./data/train/ham'):\n",
    "                ham_total += 1\n",
    "                x_train_ham += word_appears\n",
    "            else:\n",
    "                spam_total += 1\n",
    "                x_train_spam += word_appears\n",
    "                \n",
    "doc_total = ham_total + spam_total\n",
    "prior = np.array([ham_total / doc_total, spam_total / doc_total])\n",
    "likelihood_ham = np.log((x_train_ham + 1) / (ham_total + 2))\n",
    "likelihood_spam = np.log((x_train_spam + 1) / (spam_total + 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.a) Top10 most indicative of the SPAM class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viagra\n",
      "php\n",
      "sex\n",
      "meds\n",
      "cialis\n",
      "pills\n",
      "prescription\n",
      "medications\n",
      "wiil\n",
      "photoshop\n"
     ]
    }
   ],
   "source": [
    "print(\"Top10 most indicative of the SPAM class:\")\n",
    "for i in np.argsort(likelihood_spam/likelihood_ham)[:10]:\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk('./data/test'):\n",
    "    dirpath = dirpath.replace('\\\\', \"/\")\n",
    "    for f in filenames:\n",
    "        if f.endswith('.txt'):\n",
    "            if dirpath.startswith('./data/test/ham'):\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "            word_appears = np.zeros(len(all_word))\n",
    "            for x in open(path.join(dirpath, f), encoding='utf-8', errors='ignore').read().lower().split():\n",
    "                if x.isalpha() and x in all_word and len(x) > 1 and word_appears[word2id[x]]==0:\n",
    "                    word_appears[word2id[x]] = 1\n",
    "                        \n",
    "            p_ham = (word_appears@likelihood_ham)+np.log(prior[0])\n",
    "            p_spam = (word_appears@likelihood_spam)+np.log(prior[1])\n",
    "            if p_ham<p_spam:\n",
    "                preds.append(0)\n",
    "            else:\n",
    "                preds.append(1)\n",
    "\n",
    "labels = np.array(labels)\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.b) Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9859733978234583\n"
     ]
    }
   ],
   "source": [
    "acc = 1 - (labels^preds).sum()/len(preds)\n",
    "print(\"Accuracy: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.d) Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9810469314079422  Recall:  0.9670818505338078\n"
     ]
    }
   ],
   "source": [
    "tp,fn,fp,tn = list(confusion_matrix(labels,preds).reshape(-1))\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "print(\"Precision: \",precision,\" Recall: \",recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
